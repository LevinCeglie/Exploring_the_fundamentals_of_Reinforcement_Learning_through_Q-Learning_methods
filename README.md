# Exploring the fundamentals of Reinforcement Learning through Q-Learning methods

This repository contains my MaturaPaper as well as the all the code necessary. There are also fully trained snake-agents included.

## About my MaturaPaper
This work discusses and answers two hypotheses. The first being the following: \textit{One of the most basic solution methods of reinforcement learning, namely tabular Q-Learning, is adequate to solve the Cartpole environment, provided by OpenAI-Gym.} And the second: \textit{Using deep reinforcement learning, an agent can learn to play the game of Snake on a human-level performance.} As stated in the hypotheses themselves, they lie within the field of reinforcement learning. Therefore, this work begins by exploring and explaining the theory underlying reinforcement learning. This work will conduct three different experiments to not only answer the stated hypotheses, but also to improve the understanding of the presented algorithm.
		
We start by understanding what the concept behind reinforcement learning is and set the foundations for this work with a mathematical framework called the Markov decision process (MDP). The MDP is the base for the different solution methods. The first one being tabular Q-learning. To improve our understanding of said solution method, we start by solving a simple gridworld environment, where we discover the immense importance of parameter optimization.
		
We apply the same tabular Q-learning algorithm to the Cartpole environment provided by OpenAI-Gym. OpenAI-Gym is a programming library, written in python, for developing and comparing reinforcement learning algorithms. With the parameter optimization method explained in the first experiment (i.e., the gridworld environment), we manage to find a set of parameters, with which the agent is able to solve the Cartpole environment in only 164 episodes. That result confirms the first hypothesis.
		
Before we tackle the second hypothesis, we look into artificial neural networks. We first discuss their role in reinforcement learning, followed by an overview of their structure. Then we dive deeply into the details of how an artificial neural network functions. We also learn the basics of optimizing an artificial neural network to assume a given function. With that knowledge, we incorporate artificial neural networks into our existing tabular Q-learning algorithm and thus end up with a deep Q-learning algorithm, which allows greater complexity.
		
Finally, we set up the Snake game as to fit in the framework of an MDP. Most interesting is the state we choose to return to the agent. We find that there are two options when thinking about the state representation given to the agent, a top-down view or a 2D vision, and within those two categories, there are again many different options. We continue by optimizing the agent's parameters as best as possible within a reasonable timeframe. With those optimized parameters, we train sixteen different agents using eight different state representations. Besides the significant performance differences between the state representations, we find that most agents were able to outperform a human test group, consisting of fifteen people, by a very significant margin. To be more specific our best agent averaged a score of 27 on a nine by nine grid, which is equivalent to filling 35\% of the grid. Whereas, the human test group achieved an average score of 1.1 that is equivalent to filling 3\% of the grid. Furthermore, our results surpassed ones from literature, as their agent filled 8\% of the grid on average. These results confirm our second hypothesis.